---
title: "HW4"
author: "Mark Mitri"
date: "09/12/2022"
output: html_notebook
---

Instructions: Submit an R notebook that does what each of the following questions requires.

Linear models


1) (3 points)

![The versicolor iris](https://upload.wikimedia.org/wikipedia/commons/2/27/Blue_Flag%2C_Ottawa.jpg)

We have seen the Iris dataset previously.  Make a copy of the iris dataframe, and with the copy, create a new column called "class" that is TRUE when Species matches "versicolor" and FALSE otherwise.  Then delete the Species column.  This sets up the data to be used in building a versicolor classifier.

```{r}
irisCopy <- data.frame(iris)
irisCopy$class <- ifelse(irisCopy$Species == "versicolor", irisCopy$class <- TRUE, irisCopy$class <- FALSE)
irisCopy$Species <- NULL
irisCopy
```

Now build a logistic regression model using the dataframe you have created.  Use that model to predict the probability of the class variable for the data used in training.
```{r}
#YOUR CODE GOES HERE
logModel <- glm(irisCopy$class ~ irisCopy$Sepal.Length + irisCopy$Sepal.Width + irisCopy$Petal.Length + irisCopy$Petal.Width, family = binomial);
summary(logModel)
logModelPred <- predict(logModel, type = "response")
logModelPred[1:5]
```

Now convert the predicted probability vector into a class prediction vector (using a threshold).
Compare the class prediction vector with the actual class labels and calculate the accuracy of this model (e.g., how well it has learned the training data).
```{r}
#YOUR CODE GOES HERE
# .4 threshold
threshold <- .4
classPred <- logModelPred > threshold
count <- 0
index <- 1
for(x in irisCopy$class){
  if(x == classPred[index]){
    count <- count + 1
  }
  index <- index + 1
}
accuracy <- count/length(irisCopy$class)
accuracy
```


2)  (2 points)

Let's see if we can improve the performance you found above.  Add a new feature that is a nonlinear combination of one or more existing features.  Rebuild your model and evaluate it.  Does it perform better?

```{r}
#YOUR CODE GOES HERE
irisCopy$Sepal.Width2 <- (irisCopy$Sepal.Width)^2
logModel2 <- glm(irisCopy$class ~ irisCopy$Sepal.Length + irisCopy$Sepal.Width + irisCopy$Petal.Length + irisCopy$Petal.Width + irisCopy$Sepal.Width2, family = binomial)
logModelPred2 <- predict(logModel2, type = "response")

threshold <- .4
classPred2 <- logModelPred2 > threshold
count <- 0
index <- 1
for(x in irisCopy$class){
  if(x == classPred2[index]){
    count <- count + 1
  }
  index <- index + 1
}
accuracy <- count/length(irisCopy$class)
accuracy
```

Add a second nonlinear feature.  Can it improve things further?

```{r}
#YOUR CODE GOES HERE
irisCopy$WidthLength <- (irisCopy$Sepal.Width)*(irisCopy$Petal.Length)
logModel3 <- glm(irisCopy$class ~ irisCopy$Sepal.Length + irisCopy$Sepal.Width + irisCopy$Petal.Length + irisCopy$Petal.Width + irisCopy$Sepal.Width2 + irisCopy$WidthLength, family = binomial)
logModelPred3 <- predict(logModel3, type = "response") 

threshold <- .4
classPred3 <- logModelPred3 > threshold
count <- 0
index <- 1
for(x in irisCopy$class){
  if(x == classPred3[index]){
    count <- count + 1
  }
  index <- index + 1
}
accuracy <- count/length(irisCopy$class)
accuracy
```


3)  (2 points)

Install the library "e1071".  Use the `svm()` function with a basic "linear" kernel and `type="C-classification"` and perform the same versicolor classification task as above.  How does it compare?

```{r}
#install.packages("e1071")
library(e1071)
#YOUR CODE GOES HERE
svModel <- svm(irisCopy$class ~ irisCopy$Sepal.Length + irisCopy$Sepal.Width + irisCopy$Petal.Length + irisCopy$Petal.Width + irisCopy$Sepal.Width2 + irisCopy$WidthLength, type = "C-classification")
svModelPred <- predict(svModel, type = "response") 

threshold <- .4
classPred3 <- svModelPred > threshold
count <- 0
index <- 1
for(x in irisCopy$class){
  if(x == svModelPred[index]){
    count <- count + 1
  }
  index <- index + 1
}
accuracy <- count/length(irisCopy$class)
accuracy
```

What if you use a different kernel?  Can you get significantly better performance?

```{r}
#YOUR CODE GOES HERE
svModel <- svm(irisCopy$class ~ irisCopy$Sepal.Length + irisCopy$Sepal.Width + irisCopy$Petal.Length + irisCopy$Petal.Width + irisCopy$Sepal.Width2 + irisCopy$WidthLength, type = "C-classification", kernel = "polynomial")
svModelPred <- predict(svModel, type = "response") 

threshold <- .4
classPred3 <- svModelPred > threshold
count <- 0
index <- 1
for(x in irisCopy$class){
  if(x == svModelPred[index]){
    count <- count + 1
  }
  index <- index + 1
}
accuracy <- count/length(irisCopy$class)
accuracy
```

4) (2 points)

Do you think it that Sepal.Length is predictable as a function of class and the other three measurements?  (We will not use the non-linear features for the moment.) Let's build a simple linear regression model using these original features.  This also uses `glm()` but using the default `gaussian` family.  Generate the vector of predicted values (same as before).

```{r}
#YOUR CODE GOES HERE
SepalMod <- glm(irisCopy$Sepal.Length ~ irisCopy$class + irisCopy$Sepal.Width + irisCopy$Petal.Length + irisCopy$Petal.Width, family = gaussian);
summary(SepalMod)
SepalModPred <- predict(SepalMod, type = "response")
```

But how should we evaluate linear regression?  We can't use accuracy because we don't have class predictions.  A common metric is root mean squared error, defined as `sqrt(mean((firstvector - secondvector)^2))`.  Since it is an error measure, the smaller the value you get, the better it is.  What RMSE do you get for this linear regression model?

```{r}
#YOUR CODE GOES HERE
SepalPredError <- sqrt(mean((SepalModPred-irisCopy$Sepal.Length)^2))
SepalPredError
```

5) (1 point)

Can you improve upon the performance in question 4 when adding the two non-linear variables from question 2?

```{r}
#YOUR CODE GOES HERE
SepalMod <- glm(irisCopy$Sepal.Length ~ irisCopy$class + irisCopy$Sepal.Width + irisCopy$Petal.Length + irisCopy$Petal.Width + irisCopy$Sepal.Width2 + irisCopy$WidthLength, family = gaussian);
SepalModPred <- predict(SepalMod, type = "response")

threshold <- .4
classPred4 <- SepalModPred > threshold
count <- 0
index <- 1
for(x in irisCopy$Sepal.Length){
  if(x == SepalModPred[index]){
    count <- count + 1
  }
  index <- index + 1
}

SepalPredError2 <- sqrt(mean((SepalModPred-irisCopy$Sepal.Length)^2))
SepalPredError2
```
